# -*- coding: utf-8 -*-
"""Kdm_icp4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/AdityaPrasadYarlagadda/KDM-LAB4/blob/main/Kdm_icp4.ipynb

Setting up PySpark in Colab

Spark is written in the Scala programming language and requires the Java Virtual Machine (JVM) to run. Therefore, our first task is to download Java
"""

!sudo apt-get update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

"""Next, we will install Apache Spark 3.0.1 with Hadoop 2.7"""

!wget -q https://www-us.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz

"""Now, we just need to unzip that folder."""

!tar xf spark-3.0.1-bin-hadoop2.7.tgz

"""There is one last thing that we need to install and that is the findspark library. It will locate Spark on the system and import it as a regular library."""

!pip install -q findspark

"""Now that we have installed all the necessary dependencies in Colab, it is time to set the environment path. This will enable us to run Pyspark in the Colab environment."""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"

"""We need to locate Spark in the system. For that, we import findspark and use the findspark.init() method."""

import findspark
findspark.init()

"""If you want to know the location where Spark is installed, use findspark.find()"""

findspark.find()

"""Now, we can import SparkSession from pyspark.sql and create a SparkSession, which is the entry point to Spark.

You can give a name to the session using appName() and add some configurations with config() if you wish.
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder\
        .master("local")\
        .appName("Colab")\
        .config('spark.ui.port', '4050')\
        .getOrCreate()

"""Finally, print the SparkSession variable."""

spark

"""Now, load the dataset. For that I'm using the .csv module.

And passing the inferSchema parameter = true; it will enable the Spark to automatically determine the data type for each column but it has to go over the data once.If we didn't set inderShema to True, then are the tokens are considered as string.
"""

df = spark.read.csv("/content/data.csv", header=True, inferSchema= True)
df.printSchema()

"""**Performing actions on data                                                        
operation 1: Finding number of rows in a data** 
"""

df.count()

"""**operation2: Finding First Row**"""

df.first()

"""**Operation 3: Finding Required rows using take function**

---


"""

df.take(5)

"""**Performing transformations on data          
Operation 1: Replacing the data**



"""

df.replace('Male',None).show()

"""**Operation 2: Finding required data using filter**"""

df.filter(df.gender == 'Male').collect()

"""**Opearation 3: Grouping the data and showing count.**"""

op=df.groupBy('gender').count()
op.show()